<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Science Foundations Level 1 - Complete Report</title>
    <style>
        body {
            font-family: 'Times New Roman', serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background: white;
            color: #333;
        }
        
        .page {
            max-width: 8.5in;
            min-height: 11in;
            margin: 0 auto 40px;
            padding: 1in;
            background: white;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            page-break-after: always;
            position: relative;
        }
        
        .page-number {
            position: absolute;
            bottom: 0.5in;
            right: 1in;
            font-size: 12px;
        }
        
        .cover-page {
            text-align: center;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            min-height: 9in;
        }
        
        .title {
            font-size: 28px;
            font-weight: bold;
            margin: 20px 0;
            color: #1f4788;
        }
        
        .subtitle {
            font-size: 20px;
            margin: 15px 0;
        }
        
        .company-info {
            font-size: 16px;
            margin: 10px 0;
        }
        
        .student-info {
            font-size: 14px;
            margin: 30px 0;
        }
        
        h1 {
            font-size: 24px;
            color: #1f4788;
            border-bottom: 2px solid #1f4788;
            padding-bottom: 10px;
            margin-top: 30px;
        }
        
        h2 {
            font-size: 20px;
            color: #2d5aa0;
            margin-top: 25px;
        }
        
        h3 {
            font-size: 18px;
            color: #4472c4;
            margin-top: 20px;
        }
        
        .toc-item {
            display: flex;
            justify-content: space-between;
            margin: 8px 0;
            padding: 5px 0;
        }
        
        .toc-dots {
            flex-grow: 1;
            border-bottom: 1px dotted #666;
            margin: 0 10px;
            height: 1em;
        }
        
        .screenshot-placeholder {
            width: 100%;
            height: 300px;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border: 2px dashed #1f4788;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 20px 0;
            font-style: italic;
            color: #666;
        }
        
        .code-block {
            background: #f8f9fa;
            border: 1px solid #e9ecef;
            border-left: 4px solid #1f4788;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            font-size: 12px;
            overflow-x: auto;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        
        th {
            background-color: #1f4788;
            color: white;
        }
        
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        
        .certificate-box {
            border: 3px solid #1f4788;
            padding: 20px;
            margin: 20px 0;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-align: center;
        }
        
        .data-viz {
            background: linear-gradient(45deg, #ff6b6b, #4ecdc4);
            height: 250px;
            margin: 20px 0;
            border-radius: 8px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
        }
        
        .process-flow {
            display: flex;
            justify-content: space-around;
            align-items: center;
            margin: 30px 0;
        }
        
        .process-step {
            background: #1f4788;
            color: white;
            padding: 15px 20px;
            border-radius: 50px;
            margin: 0 10px;
            flex: 1;
            text-align: center;
        }
        
        .arrow {
            font-size: 24px;
            color: #1f4788;
        }
        
        @media print {
            .page {
                box-shadow: none;
                margin-bottom: 0;
            }
        }
    </style>
</head>
<body>

<!-- Page 1: Cover Page -->
<div class="page cover-page">
    <div class="certificate-box">
        <h1 style="color: white; border: none; margin: 0;">IBM DATA SCIENCE FOUNDATIONS</h1>
        <h2 style="color: white;">LEVEL 1 CERTIFICATE</h2>
    </div>
    
    <div class="title">COMPREHENSIVE REPORT ON<br>DATA SCIENCE FOUNDATIONS</div>
    <div class="subtitle">Exploring Data Analytics, Machine Learning & Business Intelligence</div>
    
    <div class="company-info">
        <strong>Certification Authority:</strong><br>
        International Business Machines Corporation (IBM)<br>
        <strong>Certificate Verification:</strong><br>
        https://www.credly.com/badges/41c610d9-9467-4184-bb82-71f56910d8b4
    </div>
    
    <div class="student-info">
        <strong>SUBMITTED BY</strong><br>
        AADI<br>
        Data Science Foundations Level 1 Certified<br>
        <strong>Issued on:</strong> July 14, 2025
    </div>
    
    <div style="margin-top: 50px;">
        <strong>Date of Submission:</strong> August 2025
    </div>
</div>

<!-- Page 2: Declaration -->
<div class="page">
    <h1>DECLARATION</h1>
    
    <p style="text-align: justify; margin-top: 40px;">
        I hereby declare that all the work presented in this report is based on my comprehensive study and practical implementation of Data Science fundamentals during my IBM Data Science Foundations Level 1 certification program. This report contains authentic documentation of theoretical knowledge, practical exercises, and project implementations completed as part of the certification requirements.
    </p>
    
    <p style="text-align: justify; margin-top: 30px;">
        The content presented herein reflects my understanding of core data science concepts including data analysis, statistical methods, machine learning algorithms, and data visualization techniques. All code implementations, analysis results, and project outcomes documented in this report are the result of my individual effort and learning during the certification period.
    </p>
    
    <p style="text-align: justify; margin-top: 30px;">
        I confirm that this report has been prepared in accordance with IBM's Data Science's guidelines and represents a comprehensive overview of the skills and knowledge acquired through this professional certification program.
    </p>
    
    <div style="margin-top: 100px;">
        <p><strong>Student Name:</strong> AADI</p>
        <p><strong>Certificate ID:</strong> 41c610d9-9467-4184-bb82-71f56910d8b4</p>
        <p><strong>Certification Date:</strong> July 14, 2025</p>
        <p><strong>Report Date:</strong> August 2025</p>
    </div>
    
    <div style="margin-top: 80px;">
        <p>_________________</p>
        <p><strong>Signature</strong></p>
    </div>
    
    <div class="page-number">2</div>
</div>

<!-- Page 3: Acknowledgment -->
<div class="page">
    <h1>ACKNOWLEDGMENT</h1>
    
    <p style="text-align: justify; margin-top: 40px;">
        I would like to express my sincere gratitude to IBM for providing such a comprehensive and well-structured Data Science Foundations Level 1 certification program. The curriculum design and learning materials have been instrumental in building a solid foundation in data science concepts and practical applications.
    </p>
    
    <p style="text-align: justify; margin-top: 30px;">
        Special thanks to the IBM team of instructors and course designers who have made complex data science concepts accessible through interactive learning modules, hands-on exercises, and real-world case studies. The structured approach to learning, from basic statistics to advanced machine learning concepts, has provided me with a comprehensive understanding of the data science landscape.
    </p>
    
    <p style="text-align: justify; margin-top: 30px;">
        I am particularly grateful for the practical approach adopted in this certification, which allowed me to work with real datasets and industry-standard tools. The hands-on experience with Python programming, statistical analysis, and data visualization has been invaluable in developing practical skills that are directly applicable in professional settings.
    </p>
    
    <p style="text-align: justify; margin-top: 30px;">
        This certification has not only enhanced my technical skills but also provided me with a clear understanding of how data science can drive business decisions and create value across various industries. The knowledge gained through this program will serve as a strong foundation for my continued growth in the field of data science.
    </p>
    
    <p style="text-align: justify; margin-top: 30px;">
        I also acknowledge the valuable resources and support provided through IBM's learning platform, which made it possible to complete this certification successfully while maintaining a balance with other professional commitments.
    </p>
    
    <div class="page-number">3</div>
</div>

<!-- Page 4: Abstract -->
<div class="page">
    <h1>ABSTRACT</h1>
    
    <p style="text-align: justify; margin-top: 40px;">
        This report presents a comprehensive overview of the IBM Data Science Foundations Level 1 certification program and the knowledge acquired through its completion. The certification covers fundamental concepts in data science, including data analysis methodologies, statistical techniques, machine learning algorithms, and data visualization principles.
    </p>
    
    <p style="text-align: justify; margin-top: 20px;">
        The modern business landscape increasingly relies on data-driven decision making, making data science skills essential for professionals across various industries. This certification addresses the growing demand for data literacy by providing foundational knowledge in data collection, cleaning, analysis, and interpretation.
    </p>
    
    <p style="text-align: justify; margin-top: 20px;">
        Key areas covered in this certification include:
    </p>
    
    <ul style="margin-top: 20px; text-align: justify;">
        <li><strong>Data Science Methodology:</strong> Understanding the data science lifecycle from problem definition to model deployment</li>
        <li><strong>Statistical Analysis:</strong> Fundamental statistical concepts for data interpretation and hypothesis testing</li>
        <li><strong>Python Programming:</strong> Essential programming skills for data manipulation and analysis</li>
        <li><strong>Data Visualization:</strong> Creating effective visual representations of data insights</li>
        <li><strong>Machine Learning Basics:</strong> Introduction to supervised and unsupervised learning algorithms</li>
        <li><strong>Business Applications:</strong> Real-world case studies demonstrating data science impact</li>
    </ul>
    
    <p style="text-align: justify; margin-top: 20px;">
        This report documents the practical implementation of learned concepts through hands-on projects, including a comprehensive customer segmentation analysis that demonstrates the application of clustering algorithms and data visualization techniques. The project showcases the end-to-end data science process from data preprocessing to actionable business insights.
    </p>
    
    <p style="text-align: justify; margin-top: 20px;">
        The certification has provided valuable skills in data manipulation, statistical analysis, and machine learning that are directly applicable to solving real-world business problems and driving data-informed decision making.
    </p>
    
    <div class="page-number">4</div>
</div>

<!-- Page 5: Table of Contents -->
<div class="page">
    <h1>TABLE OF CONTENTS</h1>
    
    <div style="margin-top: 40px;">
        <div class="toc-item">
            <span><strong>1. INTRODUCTION</strong></span>
            <span class="toc-dots"></span>
            <span><strong>6</strong></span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>1.1 About IBM Data Science Certification</span>
            <span class="toc-dots"></span>
            <span>7</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>1.2 Certification Objectives</span>
            <span class="toc-dots"></span>
            <span>8</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>1.3 Learning Outcomes</span>
            <span class="toc-dots"></span>
            <span>9</span>
        </div>
        
        <div class="toc-item">
            <span><strong>2. DATA SCIENCE FUNDAMENTALS</strong></span>
            <span class="toc-dots"></span>
            <span><strong>10</strong></span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>2.1 What is Data Science?</span>
            <span class="toc-dots"></span>
            <span>11</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>2.2 Data Science Methodology</span>
            <span class="toc-dots"></span>
            <span>12</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>2.3 Types of Data Analysis</span>
            <span class="toc-dots"></span>
            <span>13</span>
        </div>
        
        <div class="toc-item">
            <span><strong>3. STATISTICAL FOUNDATIONS</strong></span>
            <span class="toc-dots"></span>
            <span><strong>14</strong></span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>3.1 Descriptive Statistics</span>
            <span class="toc-dots"></span>
            <span>15</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>3.2 Inferential Statistics</span>
            <span class="toc-dots"></span>
            <span>16</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>3.3 Hypothesis Testing</span>
            <span class="toc-dots"></span>
            <span>17</span>
        </div>
        
        <div class="toc-item">
            <span><strong>4. PROGRAMMING FOR DATA SCIENCE</strong></span>
            <span class="toc-dots"></span>
            <span><strong>18</strong></span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>4.1 Python Fundamentals</span>
            <span class="toc-dots"></span>
            <span>19</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>4.2 Data Manipulation with Pandas</span>
            <span class="toc-dots"></span>
            <span>20</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>4.3 Numerical Computing with NumPy</span>
            <span class="toc-dots"></span>
            <span>21</span>
        </div>
        
        <div class="toc-item">
            <span><strong>5. DATA VISUALIZATION</strong></span>
            <span class="toc-dots"></span>
            <span><strong>22</strong></span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>5.1 Visualization Principles</span>
            <span class="toc-dots"></span>
            <span>23</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>5.2 Matplotlib and Seaborn</span>
            <span class="toc-dots"></span>
            <span>24</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>5.3 Interactive Visualizations</span>
            <span class="toc-dots"></span>
            <span>25</span>
        </div>
        
        <div class="toc-item">
            <span><strong>6. MACHINE LEARNING BASICS</strong></span>
            <span class="toc-dots"></span>
            <span><strong>26</strong></span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>6.1 Introduction to ML</span>
            <span class="toc-dots"></span>
            <span>27</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>6.2 Supervised Learning</span>
            <span class="toc-dots"></span>
            <span>28</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>6.3 Unsupervised Learning</span>
            <span class="toc-dots"></span>
            <span>29</span>
        </div>
        
        <div class="toc-item">
            <span><strong>7. CAPSTONE PROJECT</strong></span>
            <span class="toc-dots"></span>
            <span><strong>30</strong></span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>7.1 Project Overview</span>
            <span class="toc-dots"></span>
            <span>31</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>7.2 Data Analysis Process</span>
            <span class="toc-dots"></span>
            <span>32</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>7.3 Results and Insights</span>
            <span class="toc-dots"></span>
            <span>33</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>7.4 Project Screenshots and Code</span>
            <span class="toc-dots"></span>
            <span>34</span>
        </div>
        
        <div class="toc-item">
            <span><strong>8. CONCLUSION & FUTURE LEARNING</strong></span>
            <span class="toc-dots"></span>
            <span><strong>35</strong></span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>8.1 Key Takeaways</span>
            <span class="toc-dots"></span>
            <span>36</span>
        </div>
        
        <div class="toc-item">
            <span><strong>9. REFERENCES</strong></span>
            <span class="toc-dots"></span>
            <span><strong>37</strong></span>
        </div>
        
        <div class="toc-item">
            <span><strong>10. APPENDICES</strong></span>
            <span class="toc-dots"></span>
            <span><strong>38</strong></span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>Appendix A: Additional Code Samples</span>
            <span class="toc-dots"></span>
            <span>39</span>
        </div>
        <div class="toc-item" style="margin-left: 20px;">
            <span>Appendix B: Glossary</span>
            <span class="toc-dots"></span>
            <span>40</span>
        </div>
    </div>
    
    <div class="page-number">5</div>
</div>

<!-- Page 6: Introduction -->
<div class="page">
    <h1>1. INTRODUCTION</h1>
    
    <p style="text-align: justify; margin-top: 30px;">
        Data science has emerged as one of the most critical disciplines in the modern digital economy. As organizations generate unprecedented amounts of data, the ability to extract meaningful insights and drive decision-making through data analysis has become a competitive advantage across industries.
    </p>
    
    <p style="text-align: justify; margin-top: 20px;">
        The IBM Data Science Foundations Level 1 certification represents a comprehensive introduction to the field of data science, covering essential concepts, methodologies, and tools required to begin a career in data analytics. This certification is designed to provide learners with both theoretical understanding and practical skills necessary to tackle real-world data challenges.
    </p>
    
    <h2>Overview of Data Science</h2>
    
    <p style="text-align: justify;">
        Data science is an interdisciplinary field that combines statistics, programming, domain expertise, and business acumen to extract knowledge and insights from structured and unstructured data. The field encompasses various techniques including:
    </p>
    
    <ul style="margin-top: 15px;">
        <li><strong>Data Collection and Cleaning:</strong> Gathering and preparing data for analysis</li>
        <li><strong>Exploratory Data Analysis:</strong> Understanding data patterns and relationships</li>
        <li><strong>Statistical Modeling:</strong> Applying mathematical models to explain phenomena</li>
        <li><strong>Machine Learning:</strong> Building predictive models and algorithms</li>
        <li><strong>Data Visualization:</strong> Creating compelling visual representations of insights</li>
        <li><strong>Business Intelligence:</strong> Translating findings into actionable business strategies</li>
    </ul>
    
    <h2>Industry Relevance</h2>
    
    <p style="text-align: justify;">
        According to recent industry reports, data science roles are among the fastest-growing career opportunities globally. Organizations across healthcare, finance, technology, retail, and manufacturing are investing heavily in data science capabilities to:
    </p>
    
    <ul>
        <li>Improve customer experience and personalization</li>
        <li>Optimize operational efficiency</li>
        <li>Identify new market opportunities</li>
        <li>Reduce costs and risks</li>
        <li>Enable data-driven decision making at all levels</li>
    </ul>
    
    <div class="screenshot-placeholder">
        [Screenshot: IBM Data Science Platform Interface]
    </div>
    
    <div class="page-number">6</div>
</div>

<!-- Page 7: About IBM Data Science Certification -->
<div class="page">
    <h2>1.1 About IBM Data Science Certification</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        IBM's Data Science Foundations Level 1 certification is part of IBM's comprehensive professional certification program designed to validate skills and knowledge in emerging technologies. This certification serves as an entry point into the data science field and is recognized globally by employers seeking candidates with fundamental data science competencies.
    </p>
    
    <h3>Certification Structure</h3>
    
    <p style="text-align: justify;">
        The certification program is structured around core competencies essential for data science practitioners:
    </p>
    
    <div style="background: #f8f9fa; padding: 20px; margin: 20px 0; border-left: 4px solid #1f4788;">
        <h4>Core Modules:</h4>
        <ul>
            <li><strong>Module 1:</strong> Data Science Methodology and Lifecycle</li>
            <li><strong>Module 2:</strong> Data Collection and Preparation Techniques</li>
            <li><strong>Module 3:</strong> Exploratory Data Analysis and Statistics</li>
            <li><strong>Module 4:</strong> Introduction to Machine Learning</li>
            <li><strong>Module 5:</strong> Data Visualization and Communication</li>
            <li><strong>Module 6:</strong> Practical Applications and Case Studies</li>
        </ul>
    </div>
    
    <h3>Learning Format</h3>
    
    <p style="text-align: justify;">
        The certification combines multiple learning modalities to ensure comprehensive understanding:
    </p>
    
    <table>
        <tr>
            <th>Learning Component</th>
            <th>Description</th>
            <th>Duration</th>
        </tr>
        <tr>
            <td>Interactive Lectures</td>
            <td>Video-based content with expert instructors</td>
            <td>15 hours</td>
        </tr>
        <tr>
            <td>Hands-on Labs</td>
            <td>Practical exercises using real datasets</td>
            <td>20 hours</td>
        </tr>
        <tr>
            <td>Case Studies</td>
            <td>Industry-specific problem solving</td>
            <td>8 hours</td>
        </tr>
        <tr>
            <td>Assessment</td>
            <td>Comprehensive evaluation of skills</td>
            <td>2 hours</td>
        </tr>
    </table>
    
    <h3>Recognition and Validity</h3>
    
    <p style="text-align: justify;">
        The IBM Data Science Foundations Level 1 certification is:
    </p>
    
    <ul>
        <li>Globally recognized by leading technology companies</li>
        <li>Valid for three years from the date of issuance</li>
        <li>Backed by IBM's reputation as a leader in enterprise technology</li>
        <li>Integrated with IBM's comprehensive learning ecosystem</li>
        <li>Stackable toward advanced data science certifications</li>
    </ul>
    
    <div class="certificate-box">
        <h3>Certificate Details</h3>
        <p><strong>Certificate ID:</strong> 41c610d9-9467-4184-bb82-71f56910d8b4</p>
        <p><strong>Issue Date:</strong> July 14, 2025</p>
        <p><strong>Credential URL:</strong> https://www.credly.com/badges/41c610d9-9467-4184-bb82-71f56910d8b4</p>
    </div>
    
    <div class="page-number">7</div>
</div>

<!-- Page 8: Certification Objectives -->
<div class="page">
    <h2>1.2 Certification Objectives</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        The IBM Data Science Foundations Level 1 certification is designed with specific learning objectives that align with industry requirements and prepare learners for entry-level positions in data science and analytics roles.
    </p>
    
    <h3>Primary Learning Objectives</h3>
    
    <div class="process-flow">
        <div class="process-step">
            <strong>Foundation Building</strong><br>
            Establish core concepts
        </div>
        <div class="arrow">→</div>
        <div class="process-step">
            <strong>Skill Development</strong><br>
            Hands-on practice
        </div>
        <div class="arrow">→</div>
        <div class="process-step">
            <strong>Application</strong><br>
            Real-world projects
        </div>
    </div>
    
    <h3>Detailed Objectives</h3>
    
    <h4>1. Understand Data Science Fundamentals</h4>
    <ul>
        <li>Define data science and its role in modern business</li>
        <li>Identify key components of the data science ecosystem</li>
        <li>Recognize different types of data and analysis approaches</li>
        <li>Understand the data science methodology and lifecycle</li>
    </ul>
    
    <h4>2. Develop Statistical Literacy</h4>
    <ul>
        <li>Master descriptive statistics and data summarization</li>
        <li>Understand probability distributions and sampling</li>
        <li>Apply inferential statistics for decision making</li>
        <li>Conduct hypothesis testing and interpret results</li>
    </ul>
    
    <h4>3. Acquire Programming Skills</h4>
    <ul>
        <li>Learn Python programming fundamentals for data science</li>
        <li>Master data manipulation using Pandas library</li>
        <li>Perform numerical computing with NumPy</li>
        <li>Implement basic algorithms and functions</li>
    </ul>
    
    <h4>4. Create Effective Visualizations</h4>
    <ul>
        <li>Design clear and compelling data visualizations</li>
        <li>Use appropriate chart types for different data scenarios</li>
        <li>Apply visualization best practices and principles</li>
        <li>Create interactive dashboards and reports</li>
    </ul>
    
    <h4>5. Introduction to Machine Learning</h4>
    <ul>
        <li>Understand supervised and unsupervised learning concepts</li>
        <li>Implement basic classification and regression algorithms</li>
        <li>Evaluate model performance and accuracy</li>
        <li>Recognize appropriate use cases for different algorithms</li>
    </ul>
    
    <h4>6. Develop Business Acumen</h4>
    <ul>
        <li>Translate business problems into data science questions</li>
        <li>Communicate findings to non-technical stakeholders</li>
        <li>Understand ethical considerations in data science</li>
        <li>Apply data science solutions to real business scenarios</li>
    </ul>
    
    <div class="screenshot-placeholder">
        [Screenshot: IBM Learning Platform - Course Progress Dashboard]
    </div>
    
    <div class="page-number">8</div>
</div>

<!-- Page 9: Learning Outcomes -->
<div class="page">
    <h2>1.3 Learning Outcomes</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Upon successful completion of the IBM Data Science Foundations Level 1 certification, learners will have achieved specific competencies that demonstrate their readiness to contribute to data science projects and teams.
    </p>
    
    <h3>Technical Competencies</h3>
    
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; margin: 20px 0; border-radius: 8px;">
        <h4 style="color: white; margin-top: 0;">Core Technical Skills Acquired:</h4>
        <ul>
            <li>Python programming for data analysis and manipulation</li>
            <li>Statistical analysis and hypothesis testing</li>
            <li>Data visualization using multiple tools and libraries</li>
            <li>Basic machine learning algorithm implementation</li>
            <li>Data cleaning and preprocessing techniques</li>
            <li>Exploratory data analysis methodologies</li>
        </ul>
    </div>
    
    <h3>Professional Competencies</h3>
    
    <table>
        <tr>
            <th>Competency Area</th>
            <th>Specific Skills</th>
            <th>Application Level</th>
        </tr>
        <tr>
            <td>Problem Solving</td>
            <td>Define data science problems, identify appropriate methodologies</td>
            <td>Foundational</td>
        </tr>
        <tr>
            <td>Communication</td>
            <td>Present findings to technical and non-technical audiences</td>
            <td>Intermediate</td>
        </tr>
        <tr>
            <td>Critical Thinking</td>
            <td>Evaluate data quality, interpret results, assess model performance</td>
            <td>Foundational</td>
        </tr>
        <tr>
            <td>Project Management</td>
            <td>Plan and execute end-to-end data science projects</td>
            <td>Basic</td>
        </tr>
        <tr>
            <td>Ethical Reasoning</td>
            <td>Understand privacy, bias, and ethical considerations</td>
            <td>Foundational</td>
        </tr>
    </table>
    
    <h3>Career Readiness Indicators</h3>
    
    <p style="text-align: justify;">
        Graduates of this certification program demonstrate readiness for entry-level positions including:
    </p>
    
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
        <div style="background: #f8f9fa; padding: 15px; border-left: 4px solid #1f4788;">
            <h4>Technical Roles</h4>
            <ul>
                <li>Junior Data Analyst</li>
                <li>Data Science Associate</li>
                <li>Business Intelligence Analyst</li>
                <li>Research Analyst</li>
            </ul>
        </div>
        <div style="background: #f8f9fa; padding: 15px; border-left: 4px solid #2d5aa0;">
            <h4>Business Roles</h4>
            <ul>
                <li>Business Analyst</li>
                <li>Market Research Analyst</li>
                <li>Operations Analyst</li>
                <li>Data-Driven Consultant</li>
            </ul>
        </div>
    </div>
    
    <h3>Continuing Education Pathways</h3>
    
    <p style="text-align: justify;">
        This foundational certification serves as a stepping stone to advanced specializations:
    </p>
    
    <ul>
        <li><strong>IBM Data Science Professional Certificate:</strong> Advanced machine learning and AI</li>
        <li><strong>IBM AI Engineering Professional Certificate:</strong> Deep learning and neural networks</li>
        <li><strong>IBM Applied Data Science with Python:</strong> Specialized Python applications</li>
        <li><strong>Industry-Specific Certifications:</strong> Healthcare, finance, marketing analytics</li>
    </ul>
    
    <div class="screenshot-placeholder">
        [Screenshot: IBM Credential Pathway - Course Next Steps Recommendations]
    </div>
    
    <div class="page-number">9</div>
</div>

<!-- Page 10: Data Science Fundamentals -->
<div class="page">
    <h1>2. DATA SCIENCE FUNDAMENTALS</h1>
    
    <p style="text-align: justify; margin-top: 30px;">
        Data science fundamentals form the cornerstone of all analytical work and decision-making processes in modern organizations. This section explores the core concepts, methodologies, and frameworks that define the data science discipline.
    </p>
    
    <h2>The Evolution of Data Science</h2>
    
    <p style="text-align: justify;">
        Data science has evolved from traditional statistics and computer science to become a distinct interdisciplinary field. The convergence of several factors has contributed to its emergence:
    </p>
    
    <ul>
        <li><strong>Big Data:</strong> Exponential growth in data volume, velocity, and variety</li>
        <li><strong>Computing Power:</strong> Advanced processors and cloud computing capabilities</li>
        <li><strong>Algorithm Development:</strong> Sophisticated machine learning and AI algorithms</li>
        <li><strong>Business Need:</strong> Demand for data-driven competitive advantages</li>
    </ul>
    
    <div class="data-viz">
        Data Science = Statistics + Computer Science + Domain Expertise + Business Acumen
    </div>
    
    <h2>Key Characteristics of Data Science</h2>
    
    <h3>1. Interdisciplinary Nature</h3>
    <p style="text-align: justify;">
        Data science draws from multiple disciplines including mathematics, statistics, computer science, information theory, and domain-specific knowledge. This interdisciplinary approach enables comprehensive analysis of complex problems.
    </p>
    
    <h3>2. Problem-Solving Orientation</h3>
    <p style="text-align: justify;">
        Unlike traditional research that may be exploratory, data science is fundamentally problem-solving oriented. It begins with business questions and works toward actionable solutions.
    </p>
    
    <h3>3. Data-Driven Approach</h3>
    <p style="text-align: justify;">
        Decisions and conclusions are based on empirical evidence derived from data rather than intuition or assumption. This approach reduces bias and increases reliability of outcomes.
    </p>
    
    <h3>4. Iterative Process</h3>
    <p style="text-align: justify;">
        Data science follows an iterative methodology where insights from one phase inform and refine subsequent phases. This cyclical approach ensures continuous improvement and refinement of solutions.
    </p>
    
    <h2>Core Components</h2>
    
    <table>
        <tr>
            <th>Component</th>
            <th>Description</th>
            <th>Key Activities</th>
        </tr>
        <tr>
            <td>Data Engineering</td>
            <td>Infrastructure and pipelines for data processing</td>
            <td>Collection, storage, cleaning, transformation</td>
        </tr>
        <tr>
            <td>Analysis & Modeling</td>
            <td>Statistical analysis and machine learning</td>
            <td>EDA, hypothesis testing, model building</td>
        </tr>
        <tr>
            <td>Visualization</td>
            <td>Graphical representation of insights</td>
            <td>Charts, dashboards, interactive reports</td>
        </tr>
        <tr>
            <td>Communication</td>
            <td>Translating findings to stakeholders</td>
            <td>Presentations, reports, recommendations</td>
        </tr>
    </table>
    
    <div class="page-number">10</div>
</div>

<!-- Page 11: What is Data Science? -->
<div class="page">
    <h2>2.1 What is Data Science?</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Data science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines aspects of mathematics, statistics, computer science, and information science with domain expertise to solve complex problems.
    </p>
    
    <h3>Formal Definition</h3>
    
    <div style="background: #f8f9fa; padding: 20px; margin: 20px 0; border: 1px solid #1f4788; border-radius: 8px;">
        <p style="font-style: italic; text-align: center; margin: 0;">
            "Data science is an interdisciplinary field about processes and systems to extract knowledge or insights from data in various forms, either structured or unstructured, which is a continuation of some of the data analysis fields such as statistics, data mining, and predictive analytics."
        </p>
    </div>
    
    <h3>The Data Science Ecosystem</h3>
    
    <div class="process-flow">
        <div class="process-step">Data Sources</div>
        <div class="arrow">→</div>
        <div class="process-step">Data Processing</div>
        <div class="arrow">→</div>
        <div class="process-step">Analysis</div>
        <div class="arrow">→</div>
        <div class="process-step">Insights</div>
        <div class="arrow">→</div>
        <div class="process-step">Actions</div>
    </div>
    
    <h3>Types of Data in Data Science</h3>
    
    <h4>Structured Data</h4>
    <ul>
        <li>Organized in rows and columns (databases, spreadsheets)</li>
        <li>Easily searchable and analyzable</li>
        <li>Examples: Customer records, financial transactions, survey responses</li>
    </ul>
    
    <h4>Semi-Structured Data</h4>
    <ul>
        <li>Contains organizational properties but not in tabular format</li>
        <li>Examples: JSON files, XML documents, emails</li>
        <li>Requires parsing and transformation for analysis</li>
    </ul>
    
    <h4>Unstructured Data</h4>
    <ul>
        <li>No predefined format or organization</li>
        <li>Examples: Text documents, images, videos, social media posts</li>
        <li>Requires advanced techniques for processing and analysis</li>
    </ul>
    
    <h3>Data Science vs. Related Fields</h3>
    
    <table>
        <tr>
            <th>Field</th>
            <th>Focus</th>
            <th>Key Differences</th>
        </tr>
        <tr>
            <td>Statistics</td>
            <td>Mathematical analysis of data</td>
            <td>More theoretical, less emphasis on programming</td>
        </tr>
        <tr>
            <td>Business Intelligence</td>
            <td>Historical data analysis for reporting</td>
            <td>Descriptive analytics, structured data focus</td>
        </tr>
        <tr>
            <td>Machine Learning</td>
            <td>Algorithm development for prediction</td>
            <td>Subset of data science, technical focus</td>
        </tr>
        <tr>
            <td>Data Analytics</td>
            <td>Examining data to draw conclusions</td>
            <td>Narrower scope, less predictive modeling</td>
        </tr>
    </table>
    
    <h3>Applications Across Industries</h3>
    
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
        <div style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
            <h4>Healthcare</h4>
            <ul>
                <li>Drug discovery and development</li>
                <li>Personalized treatment plans</li>
                <li>Epidemic tracking and prevention</li>
                <li>Medical image analysis</li>
            </ul>
        </div>
        <div style="background: #f3e5f5; padding: 15px; border-radius: 8px;">
            <h4>Finance</h4>
            <ul>
                <li>Fraud detection and prevention</li>
                <li>Risk assessment and management</li>
                <li>Algorithmic trading</li>
                <li>Credit scoring</li>
            </ul>
        </div>
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Data Science Project Lifecycle Diagram]
    </div>
    
    <div class="page-number">11</div>
</div>

<!-- Page 12: Data Science Methodology -->
<div class="page">
    <h2>2.2 Data Science Methodology</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        The data science methodology provides a structured approach to solving data science problems. It ensures systematic thinking and comprehensive coverage of all aspects of a data science project from conception to deployment.
    </p>
    
    <h3>The CRISP-DM Framework</h3>
    
    <p style="text-align: justify;">
        Cross Industry Standard Process for Data Mining (CRISP-DM) is the most widely adopted methodology for data science projects. It consists of six interconnected phases:
    </p>
    
    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin: 30px 0;">
        <div style="background: #1f4788; color: white; padding: 20px; border-radius: 8px; text-align: center;">
            <h4 style="color: white; margin-top: 0;">1. Business Understanding</h4>
            <p>Define objectives, assess situation, determine data mining goals</p>
        </div>
        <div style="background: #2d5aa0; color: white; padding: 20px; border-radius: 8px; text-align: center;">
            <h4 style="color: white; margin-top: 0;">2. Data Understanding</h4>
            <p>Collect initial data, describe and explore data quality</p>
        </div>
        <div style="background: #4472c4; color: white; padding: 20px; border-radius: 8px; text-align: center;">
            <h4 style="color: white; margin-top: 0;">3. Data Preparation</h4>
            <p>Select, clean, construct, integrate, and format data</p>
        </div>
        <div style="background: #5b89d6; color: white; padding: 20px; border-radius: 8px; text-align: center;">
            <h4 style="color: white; margin-top: 0;">4. Modeling</h4>
            <p>Select modeling techniques, build and assess models</p>
        </div>
        <div style="background: #70a0e8; color: white; padding: 20px; border-radius: 8px; text-align: center;">
            <h4 style="color: white; margin-top: 0;">5. Evaluation</h4>
            <p>Evaluate results, review process, determine next steps</p>
        </div>
        <div style="background: #85b7fa; color: white; padding: 20px; border-radius: 8px; text-align: center;">
            <h4 style="color: white; margin-top: 0;">6. Deployment</h4>
            <p>Plan deployment, monitor and maintain, produce final report</p>
        </div>
    </div>
    
    <h3>Detailed Phase Breakdown</h3>
    
    <h4>Phase 1: Business Understanding</h4>
    <ul>
        <li><strong>Determine business objectives:</strong> Understand what the business wants to accomplish</li>
        <li><strong>Assess situation:</strong> Inventory resources, requirements, assumptions, and risks</li>
        <li><strong>Determine data mining goals:</strong> Define technical objectives</li>
        <li><strong>Produce project plan:</strong> Select technologies and tools, define timeline</li>
    </ul>
    
    <h4>Phase 2: Data Understanding</h4>
    <ul>
        <li><strong>Collect initial data:</strong> Acquire data and load it into analysis tools</li>
        <li><strong>Describe data:</strong> Examine gross properties like data format, quantity, identities</li>
        <li><strong>Explore data:</strong> Dig deeper into the data, query it, visualize it, identify patterns</li>
        <li><strong>Verify data quality:</strong> Examine quality issues like missing values, duplicate records</li>
    </ul>
    
    <h4>Phase 3: Data Preparation</h4>
    <ul>
        <li><strong>Select data:</strong> Choose relevant data for analysis</li>
        <li><strong>Clean data:</strong> Fix quality issues identified in understanding phase</li>
        <li><strong>Construct data:</strong> Create derived attributes and new records</li>
        <li><strong>Integrate data:</strong> Merge data from multiple sources</li>
        <li><strong>Format data:</strong> Make syntactic modifications for modeling tools</li>
    </ul>
    
    <div class="code-block">
# Example of data cleaning process
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('dataset.csv')

# Handle missing values
df.fillna(df.mean(), inplace=True)

# Remove duplicates
df.drop_duplicates(inplace=True)

# Standardize data types
df['date'] = pd.to_datetime(df['date'])
df['category'] = df['category'].astype('category')
    </div>
    
    <div class="page-number">12</div>
</div>

<!-- Page 13: Types of Data Analysis -->
<div class="page">
    <h2>2.3 Types of Data Analysis</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Data analysis can be categorized into different types based on the purpose and methodology employed. Understanding these categories helps in selecting appropriate techniques for specific business problems and expected outcomes.
    </p>
    
    <h3>The Four Types of Analytics</h3>
    
    <div style="margin: 30px 0;">
        <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 15px; margin: 10px 0; border-radius: 8px;">
            <h4 style="color: white; margin: 0;">1. Descriptive Analytics - "What happened?"</h4>
        </div>
        <ul>
            <li>Summarizes historical data to understand past events</li>
            <li>Uses basic statistical measures and visualizations</li>
            <li>Examples: Monthly sales reports, website traffic summaries, customer demographic analysis</li>
            <li>Tools: Dashboards, reports, basic charts and graphs</li>
        </ul>
        
        <div style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; padding: 15px; margin: 10px 0; border-radius: 8px;">
            <h4 style="color: white; margin: 0;">2. Diagnostic Analytics - "Why did it happen?"</h4>
        </div>
        <ul>
            <li>Examines data to understand causes behind past events</li>
            <li>Uses correlation analysis, hypothesis testing, regression analysis</li>
            <li>Examples: Root cause analysis of customer churn, factors affecting sales decline</li>
            <li>Tools: Statistical software, correlation matrices, drill-down capabilities</li>
        </ul>
        
        <div style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; padding: 15px; margin: 10px 0; border-radius: 8px;">
            <h4 style="color: white; margin: 0;">3. Predictive Analytics - "What will happen?"</h4>
        </div>
        <ul>
            <li>Uses historical data to forecast future trends and behaviors</li>
            <li>Employs statistical modeling and machine learning algorithms</li>
            <li>Examples: Sales forecasting, customer lifetime value prediction, demand planning</li>
            <li>Tools: Machine learning algorithms, time series analysis, regression models</li>
        </ul>
        
        <div style="background: linear-gradient(135deg, #fa709a 0%, #fee140 100%); color: white; padding: 15px; margin: 10px 0; border-radius: 8px;">
            <h4 style="color: white; margin: 0;">4. Prescriptive Analytics - "What should we do?"</h4>
        </div>
        <ul>
            <li>Recommends actions to achieve desired outcomes</li>
            <li>Uses optimization algorithms and simulation techniques</li>
            <li>Examples: Resource allocation, pricing strategies, treatment recommendations</li>
            <li>Tools: Optimization software, simulation models, decision trees</li>
        </ul>
    </div>
    
    <h3>Analytics Maturity Model</h3>
    
    <table>
        <tr>
            <th>Maturity Level</th>
            <th>Analytics Type</th>
            <th>Business Value</th>
            <th>Complexity</th>
        </tr>
        <tr>
            <td>Basic</td>
            <td>Descriptive</td>
            <td>Low to Medium</td>
            <td>Low</td>
        </tr>
        <tr>
            <td>Intermediate</td>
            <td>Diagnostic</td>
            <td>Medium</td>
            <td>Medium</td>
        </tr>
        <tr>
            <td>Advanced</td>
            <td>Predictive</td>
            <td>High</td>
            <td>High</td>
        </tr>
        <tr>
            <td>Optimized</td>
            <td>Prescriptive</td>
            <td>Very High</td>
            <td>Very High</td>
        </tr>
    </table>
    
    <h3>Choosing the Right Analysis Type</h3>
    
    <p style="text-align: justify;">
        The selection of analysis type depends on several factors:
    </p>
    
    <ul>
        <li><strong>Business objectives:</strong> What specific questions need to be answered</li>
        <li><strong>Data availability:</strong> Quality and quantity of historical data</li>
        <li><strong>Time constraints:</strong> Urgency of decision-making requirements</li>
        <li><strong>Resource capabilities:</strong> Technical skills and tools available</li>
        <li><strong>Risk tolerance:</strong> Comfort level with prediction uncertainty</li>
    </ul>
    
    <div class="screenshot-placeholder">
        [Screenshot: Analytics Framework Selection Decision Tree]
    </div>
    
    <div class="page-number">13</div>
</div>

<!-- Page 14: Statistical Foundations -->
<div class="page">
    <h1>3. STATISTICAL FOUNDATIONS</h1>
    
    <p style="text-align: justify; margin-top: 30px;">
        Statistics forms the mathematical backbone of data science, providing the theoretical framework for data analysis, hypothesis testing, and inference. A solid understanding of statistical concepts is essential for making valid conclusions from data and avoiding common analytical pitfalls.
    </p>
    
    <h2>The Role of Statistics in Data Science</h2>
    
    <p style="text-align: justify;">
        Statistics enables data scientists to:
    </p>
    
    <ul>
        <li>Summarize and describe data characteristics</li>
        <li>Make inferences about populations from samples</li>
        <li>Test hypotheses and validate assumptions</li>
        <li>Quantify uncertainty and risk</li>
        <li>Build predictive models with confidence measures</li>
    </ul>
    
    <h2>Fundamental Statistical Concepts</h2>
    
    <h3>Population vs. Sample</h3>
    
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
        <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
            <h4>Population</h4>
            <ul>
                <li>Complete set of all possible observations</li>
                <li>Parameters (μ, σ) describe population</li>
                <li>Often impossible to study entirely</li>
                <li>Example: All customers of a company</li>
            </ul>
        </div>
        <div style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
            <h4>Sample</h4>
            <ul>
                <li>Subset of the population</li>
                <li>Statistics (x̄, s) describe sample</li>
                <li>Used to make inferences about population</li>
                <li>Example: 1000 randomly selected customers</li>
            </ul>
        </div>
    </div>
    
    <h3>Types of Data</h3>
    
    <table>
        <tr>
            <th>Data Type</th>
            <th>Description</th>
            <th>Examples</th>
            <th>Statistical Methods</th>
        </tr>
        <tr>
            <td>Nominal</td>
            <td>Categories without order</td>
            <td>Colors, brands, countries</td>
            <td>Mode, frequency tables</td>
        </tr>
        <tr>
            <td>Ordinal</td>
            <td>Categories with order</td>
            <td>Ratings, education levels</td>
            <td>Median, percentiles</td>
        </tr>
        <tr>
            <td>Interval</td>
            <td>Numeric with equal intervals</td>
            <td>Temperature, dates</td>
            <td>Mean, standard deviation</td>
        </tr>
        <tr>
            <td>Ratio</td>
            <td>Numeric with true zero</td>
            <td>Height, weight, income</td>
            <td>All statistical measures</td>
        </tr>
    </table>
    
    <h3>Measures of Central Tendency</h3>
    
    <div class="code-block">
import numpy as np
from scipy import stats

# Sample data
data = [12, 15, 18, 20, 22, 25, 28, 30, 32, 35]

# Mean (average)
mean = np.mean(data)
print(f"Mean: {mean}")

# Median (middle value)
median = np.median(data)
print(f"Median: {median}")

# Mode (most frequent value)
mode = stats.mode(data)
print(f"Mode: {mode}")
    </div>
    
    <h3>Measures of Variability</h3>
    
    <ul>
        <li><strong>Range:</strong> Difference between maximum and minimum values</li>
        <li><strong>Variance:</strong> Average squared deviation from the mean</li>
        <li><strong>Standard Deviation:</strong> Square root of variance, in original units</li>
        <li><strong>Interquartile Range (IQR):</strong> Range of the middle 50% of data</li>
    </ul>
    
    <div class="screenshot-placeholder">
        [Screenshot: Statistical Distribution Visualization - Normal, Skewed, Bimodal]
    </div>
    
    <div class="page-number">14</div>
</div>

<!-- Page 15: Descriptive Statistics -->
<div class="page">
    <h2>3.1 Descriptive Statistics</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Descriptive statistics provide tools for summarizing and describing the main features of a dataset. These techniques help us understand what the data shows without making assumptions about larger populations.
    </p>
    
    <h3>Summary Statistics</h3>
    
    <div class="code-block">
import pandas as pd
import numpy as np

# Create sample dataset
np.random.seed(42)
data = {
    'age': np.random.normal(35, 10, 1000),
    'income': np.random.normal(50000, 15000, 1000),
    'satisfaction': np.random.randint(1, 6, 1000)
}
df = pd.DataFrame(data)

# Generate descriptive statistics
print(df.describe())

# Output includes:
# - Count: Number of non-null observations
# - Mean: Average value
# - Std: Standard deviation
# - Min/Max: Minimum and maximum values
# - 25%, 50%, 75%: Quartiles
    </div>
    
    <h3>Distribution Shapes</h3>
    
    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin: 20px 0;">
        <div style="background: #e8f5e8; padding: 15px; border-radius: 8px; text-align: center;">
            <h4>Normal Distribution</h4>
            <p>Symmetric, bell-shaped</p>
            <p>Mean = Median = Mode</p>
        </div>
        <div style="background: #ffe8e8; padding: 15px; border-radius: 8px; text-align: center;">
            <h4>Right Skewed</h4>
            <p>Long tail to the right</p>
            <p>Mean > Median > Mode</p>
        </div>
        <div style="background: #e8e8ff; padding: 15px; border-radius: 8px; text-align: center;">
            <h4>Left Skewed</h4>
            <p>Long tail to the left</p>
            <p>Mode > Median > Mean</p>
        </div>
    </div>
    
    <h3>Correlation Analysis</h3>
    
    <p style="text-align: justify;">
        Correlation measures the strength and direction of linear relationships between variables:
    </p>
    
    <ul>
        <li><strong>Pearson Correlation:</strong> Measures linear relationships (-1 to +1)</li>
        <li><strong>Spearman Correlation:</strong> Measures monotonic relationships</li>
        <li><strong>Kendall's Tau:</strong> Alternative rank-based correlation measure</li>
    </ul>
    
    <div class="code-block">
# Calculate correlation matrix
correlation_matrix = df.corr()
print(correlation_matrix)

# Interpretation:
# +1.0: Perfect positive correlation
# 0.0: No linear correlation
# -1.0: Perfect negative correlation
    </div>
    
    <h3>Data Visualization for Descriptive Statistics</h3>
    
    <table>
        <tr>
            <th>Chart Type</th>
            <th>Best Used For</th>
            <th>Data Type</th>
        </tr>
        <tr>
            <td>Histogram</td>
            <td>Distribution of single variable</td>
            <td>Continuous</td>
        </tr>
        <tr>
            <td>Box Plot</td>
            <td>Comparing distributions, identifying outliers</td>
            <td>Continuous</td>
        </tr>
        <tr>
            <td>Bar Chart</td>
            <td>Comparing categories</td>
            <td>Categorical</td>
        </tr>
        <tr>
            <td>Scatter Plot</td>
            <td>Relationships between two variables</td>
            <td>Continuous</td>
        </tr>
        <tr>
            <td>Heat Map</td>
            <td>Correlation matrices, pattern identification</td>
            <td>Multiple continuous</td>
        </tr>
    </table>
    
    <h3>Outlier Detection Methods</h3>
    
    <div class="code-block">
import matplotlib.pyplot as plt

# Z-score method for outlier detection
from scipy import stats

z_scores = np.abs(stats.zscore(df['income']))
outliers_zscore = df[z_scores > 3]

# IQR method for outlier detection
Q1 = df['income'].quantile(0.25)
Q3 = df['income'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers_iqr = df[(df['income'] < lower_bound) | (df['income'] > upper_bound)]

print(f"Outliers detected using Z-score: {len(outliers_zscore)}")
print(f"Outliers detected using IQR: {len(outliers_iqr)}")
    </div>
    
    <h3>Common Descriptive Statistics Applications</h3>
    
    <ul>
        <li><strong>Market Research:</strong> Customer demographic analysis and segmentation</li>
        <li><strong>Quality Control:</strong> Manufacturing process monitoring and control</li>
        <li><strong>Financial Analysis:</strong> Portfolio performance and risk assessment</li>
        <li><strong>Healthcare:</strong> Patient outcome tracking and medical research</li>
        <li><strong>Education:</strong> Student performance evaluation and curriculum assessment</li>
    </ul>
    
    <div class="screenshot-placeholder">
        [Screenshot: Comprehensive Dashboard with Multiple Descriptive Statistics Visualizations]
    </div>
    
    <div class="page-number">15</div>
</div>

<!-- Page 16: Inferential Statistics -->
<div class="page">
    <h2>3.2 Inferential Statistics</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Inferential statistics allows us to make conclusions about populations based on sample data. This branch of statistics is crucial for data science as it enables us to generalize findings beyond our immediate dataset and make predictions with quantified uncertainty.
    </p>
    
    <h3>Sampling Distributions</h3>
    
    <p style="text-align: justify;">
        A sampling distribution is the distribution of a statistic calculated from multiple samples of the same size from the same population. Understanding sampling distributions is fundamental to inferential statistics.
    </p>
    
    <h4>Central Limit Theorem</h4>
    <div style="background: #f8f9fa; padding: 20px; margin: 20px 0; border-left: 4px solid #1f4788;">
        <p><strong>Key Principle:</strong> Regardless of the population distribution shape, the sampling distribution of the sample mean approaches a normal distribution as the sample size increases (typically n ≥ 30).</p>
        
        <p><strong>Implications:</strong></p>
        <ul>
            <li>Sample means will be normally distributed around the population mean</li>
            <li>Standard error decreases as sample size increases</li>
            <li>Enables use of normal distribution for inference</li>
        </ul>
    </div>
    
    <h3>Confidence Intervals</h3>
    
    <p style="text-align: justify;">
        Confidence intervals provide a range of values that likely contain the population parameter with a specified level of confidence.
    </p>
    
    <div class="code-block">
import scipy.stats as stats
import numpy as np

# Calculate 95% confidence interval for population mean
sample_data = np.random.normal(100, 15, 50)  # Sample of size 50
sample_mean = np.mean(sample_data)
sample_std = np.std(sample_data, ddof=1)
n = len(sample_data)

# Standard error
se = sample_std / np.sqrt(n)

# 95% confidence interval
confidence_level = 0.95
alpha = 1 - confidence_level
t_critical = stats.t.ppf(1 - alpha/2, df=n-1)

margin_of_error = t_critical * se
ci_lower = sample_mean - margin_of_error
ci_upper = sample_mean + margin_of_error

print(f"95% Confidence Interval: ({ci_lower:.2f}, {ci_upper:.2f})")
    </div>
    
    <h3>Common Confidence Levels</h3>
    
    <table>
        <tr>
            <th>Confidence Level</th>
            <th>Alpha (α)</th>
            <th>Z-Critical Value</th>
        </tr>
        <tr>
            <td>90%</td>
            <td>0.10</td>
            <td>1.645</td>
        </tr>
        <tr>
            <td>95%</td>
            <td>0.05</td>
            <td>1.960</td>
        </tr>
        <tr>
            <td>99%</td>
            <td>0.01</td>
            <td>2.576</td>
        </tr>
    </table>
    
    <h3>Types of Statistical Tests</h3>
    
    <h4>One-Sample Tests</h4>
    <ul>
        <li><strong>One-sample t-test:</strong> Compare sample mean to known population mean</li>
        <li><strong>One-sample proportion test:</strong> Compare sample proportion to known population proportion</li>
    </ul>
    
    <h4>Two-Sample Tests</h4>
    <ul>
        <li><strong>Independent t-test:</strong> Compare means of two independent groups</li>
        <li><strong>Paired t-test:</strong> Compare means of matched pairs</li>
        <li><strong>Two-proportion z-test:</strong> Compare proportions between two groups</li>
    </ul>
    
    <h4>Multiple Group Tests</h4>
    <ul>
        <li><strong>ANOVA (Analysis of Variance):</strong> Compare means across multiple groups</li>
        <li><strong>Chi-square test:</strong> Test independence between categorical variables</li>
    </ul>
    
    <h3>P-Values and Statistical Significance</h3>
    
    <div style="background: #fff3cd; padding: 20px; margin: 20px 0; border: 1px solid #ffeaa7; border-radius: 8px;">
        <h4>Understanding P-Values:</h4>
        <ul>
            <li><strong>Definition:</strong> Probability of obtaining test results at least as extreme as observed, assuming null hypothesis is true</li>
            <li><strong>Common threshold:</strong> α = 0.05 (5% significance level)</li>
            <li><strong>Interpretation:</strong> p < α suggests evidence against null hypothesis</li>
            <li><strong>Limitation:</strong> Does not indicate effect size or practical significance</li>
        </ul>
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Statistical Test Selection Decision Tree]
    </div>
    
    <div class="page-number">16</div>
</div>

<!-- Page 17: Hypothesis Testing -->
<div class="page">
    <h2>3.3 Hypothesis Testing</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Hypothesis testing is a systematic method for making decisions about population parameters based on sample data. It provides a framework for determining whether observed differences or relationships are statistically significant or could reasonably be attributed to random variation.
    </p>
    
    <h3>The Hypothesis Testing Framework</h3>
    
    <div class="process-flow">
        <div class="process-step">Formulate Hypotheses</div>
        <div class="arrow">→</div>
        <div class="process-step">Choose Test</div>
        <div class="arrow">→</div>
        <div class="process-step">Set Significance Level</div>
        <div class="arrow">→</div>
        <div class="process-step">Calculate Test Statistic</div>
        <div class="arrow">→</div>
        <div class="process-step">Make Decision</div>
    </div>
    
    <h3>Step 1: Formulating Hypotheses</h3>
    
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
        <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
            <h4>Null Hypothesis (H₀)</h4>
            <ul>
                <li>Assumes no effect or difference exists</li>
                <li>Status quo assumption</li>
                <li>What we're testing against</li>
                <li>Example: μ = 50 (mean equals 50)</li>
            </ul>
        </div>
        <div style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
            <h4>Alternative Hypothesis (H₁)</h4>
            <ul>
                <li>Claims there is an effect or difference</li>
                <li>What we're trying to prove</li>
                <li>Can be one-tailed or two-tailed</li>
                <li>Example: μ ≠ 50 (mean not equal to 50)</li>
            </ul>
        </div>
    </div>
    
    <h3>Types of Errors</h3>
    
    <table>
        <tr>
            <th></th>
            <th>H₀ is True</th>
            <th>H₀ is False</th>
        </tr>
        <tr>
            <th>Reject H₀</th>
            <td style="background: #ffebee;"><strong>Type I Error (α)</strong><br>False Positive<br>"Crying Wolf"</td>
            <td style="background: #e8f5e8;"><strong>Correct Decision</strong><br>Power (1-β)</td>
        </tr>
        <tr>
            <th>Fail to Reject H₀</th>
            <td style="background: #e8f5e8;"><strong>Correct Decision</strong><br>Confidence (1-α)</td>
            <td style="background: #fff3e0;"><strong>Type II Error (β)</strong><br>False Negative<br>"Missing the Signal"</td>
        </tr>
    </table>
    
    <h3>Practical Example: A/B Testing</h3>
    
    <div class="code-block">
import numpy as np
from scipy import stats

# A/B Testing Example: Website Conversion Rates
# Control Group (A): Current website design
# Treatment Group (B): New website design

# Sample data
control_conversions = 45
control_visitors = 1000
treatment_conversions = 62
treatment_visitors = 1000

# Calculate conversion rates
control_rate = control_conversions / control_visitors
treatment_rate = treatment_conversions / treatment_visitors

print(f"Control conversion rate: {control_rate:.3f}")
print(f"Treatment conversion rate: {treatment_rate:.3f}")

# Hypothesis Test
# H₀: p₁ = p₂ (no difference in conversion rates)
# H₁: p₁ ≠ p₂ (there is a difference in conversion rates)

# Two-proportion z-test
from statsmodels.stats.proportion import proportions_ztest

count = np.array([control_conversions, treatment_conversions])
nobs = np.array([control_visitors, treatment_visitors])

z_stat, p_value = proportions_ztest(count, nobs)

print(f"Z-statistic: {z_stat:.3f}")
print(f"P-value: {p_value:.3f}")

# Decision
alpha = 0.05
if p_value < alpha:
    print("Reject H₀: Significant difference in conversion rates")
else:
    print("Fail to reject H₀: No significant difference")
    </div>
    
    <h3>Effect Size and Practical Significance</h3>
    
    <p style="text-align: justify;">
        Statistical significance doesn't always imply practical significance. Effect size measures help quantify the magnitude of differences:
    </p>
    
    <ul>
        <li><strong>Cohen's d:</strong> Standardized difference between two means</li>
        <li><strong>Eta squared (η²):</strong> Proportion of variance explained</li>
        <li><strong>Odds ratio:</strong> Ratio of odds between two groups</li>
        <li><strong>Correlation coefficient:</strong> Strength of linear relationship</li>
    </ul>
    
    <h3>Common Hypothesis Testing Applications</h3>
    
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
        <div style="background: #e8f5e8; padding: 15px; border-radius: 8px;">
            <h4>Business Applications</h4>
            <ul>
                <li>A/B testing for website optimization</li>
                <li>Quality control in manufacturing</li>
                <li>Marketing campaign effectiveness</li>
                <li>Customer satisfaction surveys</li>
            </ul>
        </div>
        <div style="background: #ffe8e8; padding: 15px; border-radius: 8px;">
            <h4>Research Applications</h4>
            <ul>
                <li>Clinical trial analysis</li>
                <li>Educational intervention studies</li>
                <li>Social science research</li>
                <li>Environmental impact studies</li>
            </ul>
        </div>
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Hypothesis Testing Results Dashboard with Statistical Metrics]
    </div>
    
    <div class="page-number">17</div>
</div>

<!-- Page 18: Programming for Data Science -->
<div class="page">
    <h1>4. PROGRAMMING FOR DATA SCIENCE</h1>
    
    <p style="text-align: justify; margin-top: 30px;">
        Programming skills are essential for modern data science, enabling automation, scalability, and reproducibility of analytical processes. Python has emerged as the dominant language in data science due to its simplicity, extensive libraries, and strong community support.
    </p>
    
    <h2>Why Python for Data Science?</h2>
    
    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin: 20px 0;">
        <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
            <h3>Language Advantages</h3>
            <ul>
                <li><strong>Readability:</strong> Clean, intuitive syntax</li>
                <li><strong>Versatility:</strong> Suitable for multiple domains</li>
                <li><strong>Interpreted:</strong> Interactive development</li>
                <li><strong>Cross-platform:</strong> Runs on all major OS</li>
            </ul>
        </div>
        <div style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
            <h3>Ecosystem Benefits</h3>
            <ul>
                <li><strong>Rich libraries:</strong> Specialized data science tools</li>
                <li><strong>Rich community:</strong> Continuous development</li>
                <li><strong>Industry adoption:</strong> Widely used in tech</li>
                <li><strong>Integration:</strong> Works well with other tools</li>
            </ul>
        </div>
    </div>
    
    <h2>Essential Python Data Science Stack</h2>
    
    <table>
        <tr>
            <th>Category</th>
            <th>Library</th>
            <th>Purpose</th>
            <th>Key Features</th>
        </tr>
        <tr>
            <td>Core Computing</td>
            <td>NumPy</td>
            <td>Numerical computing</td>
            <td>N-dimensional arrays, mathematical functions</td>
        </tr>
        <tr>
            <td>Data Manipulation</td>
            <td>Pandas</td>
            <td>Data analysis and manipulation</td>
            <td>DataFrames, data cleaning, merging</td>
        </tr>
        <tr>
            <td>Visualization</td>
            <td>Matplotlib</td>
            <td>Static plotting</td>
            <td>Comprehensive plotting library</td>
        </tr>
        <tr>
            <td>Visualization</td>
            <td>Seaborn</td>
            <td>Statistical visualization</td>
            <td>High-level interface, attractive defaults</td>
        </tr>
        <tr>
            <td>Machine Learning</td>
            <td>Scikit-learn</td>
            <td>Machine learning algorithms</td>
            <td>Classification, regression, clustering</td>
        </tr>
        <tr>
            <td>Statistical Analysis</td>
            <td>SciPy</td>
            <td>Scientific computing</td>
            <td>Statistical tests, optimization</td>
        </tr>
    </table>
    
    <h2>Development Environment Setup</h2>
    
    <div class="code-block">
# Installing the essential data science stack
# Using conda (recommended)
conda install numpy pandas matplotlib seaborn scikit-learn scipy jupyter

# Using pip
pip install numpy pandas matplotlib seaborn scikit-learn scipy jupyter

# Importing common libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from scipy import stats
    </div>
    
    <h2>Jupyter Notebooks for Data Science</h2>
    
    <p style="text-align: justify;">
        Jupyter Notebooks provide an interactive computing environment that combines code execution, rich text, equations, and visualizations in a single document. They are ideal for:
    </p>
    
    <ul>
        <li><strong>Exploratory Data Analysis:</strong> Interactive data exploration and visualization</li>
        <li><strong>Prototyping:</strong> Rapid development and testing of ideas</li>
        <li><strong>Documentation:</strong> Combining code with explanatory text</li>
        <li><strong>Sharing:</strong> Easy collaboration and presentation of results</li>
        <li><strong>Education:</strong> Step-by-step learning and teaching</li>
    </ul>
    
    <h2>Data Science Programming Best Practices</h2>
    
    <div style="background: #f8f9fa; padding: 20px; margin: 20px 0; border-left: 4px solid #1f4788;">
        <h3>Code Organization</h3>
        <ul>
            <li>Use meaningful variable and function names</li>
            <li>Write modular, reusable code</li>
            <li>Add comments and docstrings for documentation</li>
            <li>Follow PEP 8 style guidelines</li>
        </ul>
        
        <h3>Data Handling</h3>
        <ul>
            <li>Always backup original data before processing</li>
            <li>Implement data validation and error handling</li>
            <li>Use version control for code and data lineage</li>
            <li>Document data sources and transformations</li>
        </ul>
        
        <h3>Analysis Workflow</h3>
        <ul>
            <li>Start with exploratory data analysis</li>
            <li>Validate assumptions before applying methods</li>
            <li>Use reproducible analysis workflows</li>
            <li>Test code with sample data before full execution</li>
        </ul>
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Jupyter Notebook Interface with Data Science Code Examples]
    </div>
    
    <div class="page-number">18</div>
</div>

<!-- Page 19: Python Fundamentals -->
<div class="page">
    <h2>4.1 Python Fundamentals</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Understanding Python fundamentals is crucial for effective data science programming. This section covers essential Python concepts and syntax patterns commonly used in data science applications.
    </p>
    
    <h3>Basic Data Types and Variables</h3>
    
    <div class="code-block">
# Basic data types in Python
# Numbers
integer_var = 42
float_var = 3.14159
complex_var = 2 + 3j

# Strings
text = "Data Science"
multiline_text = """This is a
multiline string"""

# Booleans
is_data_scientist = True
is_complete = False

# Type checking
print(type(integer_var))  # <class 'int'>
print(type(text))         # <class 'str'>
    </div>
    
    <h3>Data Structures</h3>
    
    <h4>Lists - Ordered, Mutable Collections</h4>
    <div class="code-block">
# Creating and manipulating lists
numbers = [1, 2, 3, 4, 5]
mixed_list = [1, "hello", 3.14, True]

# Series operations
numbers.append(6)           # Add element
numbers.extend([7, 8])      # Add multiple elements
numbers.insert(0, 0)        # Insert at specific position
first_three = numbers[:3]   # Slicing
numbers.sort()              # Sort in place

# List comprehensions (powerful feature)
squares = [x**2 for x in range(10)]
even_squares = [x**2 for x in range(10) if x % 2 == 0]
    </div>
    
    <h4>Dictionaries - Key-Value Pairs</h4>
    <div class="code-block">
# Creating dictionaries
student = {
    "name": "John Doe",
    "age": 25,
    "grades": [85, 92, 78],
    "is_enrolled": True
}

# Dictionary operations
student["major"] = "Data Science"    # Add new key-value pair
student.update({"year": 2025})       # Update multiple values
name = student.get("name")           # Safe access
keys = list(student.keys())          # Get all keys
values = list(student.values())      # Get all values

# Dictionary comprehensions
squared_dict = {x: x**2 for x in range(5)}
    </div>
    
    <h3>Control Flow</h3>
    
    <h4>Conditional Statements</h4>
    <div class="code-block">
# If-elif-else statements
grade = 85

if grade >= 90:
    letter_grade = "A"
elif grade >= 80:
    letter_grade = "B"
elif grade >= 70:
    letter_grade = "C"
else:
    letter_grade = "F"

# Ternary operator
status = "Pass" if grade >= 70 else "Fail"
    </div>
    
    <h4>Loops</h4>
    <div class="code-block">
# For loops
for i in range(5):
    print(f"Iteration {i}")

# Iterating over collections
fruits = ["apple", "banana", "orange"]
for fruit in fruits:
    print(f"I like {fruit}")

# Enumerate for index and value
for index, fruit in enumerate(fruits):
    print(f"{index}: {fruit}")

# While loops
count = 0
while count < 5:
    print(f"Count: {count}")
    count += 1
    </div>
    
    <h3>Functions</h3>
    
    <div class="code-block">
# Basic function definition
def calculate_mean(numbers):
    """Calculate the arithmetic mean of a list of numbers."""
    if not numbers:
        return 0
    return sum(numbers) / len(numbers)

# Function with default parameters
def greet(name, greeting="Hello"):
    return f"{greeting}, {name}!"

# Lambda functions (anonymous functions)
square = lambda x: x**2
multiply = lambda x, y: x * y

# Using functions with built-in functions
numbers = [1, 2, 3, 4, 5]
squared_numbers = list(map(square, numbers))
even_numbers = list(filter(lambda x: x % 2 == 0, numbers))
    </div>
    
    <h3>Error Handling</h3>
    
    <div class="code-block">
# Try-except blocks for error handling
def safe_divide(a, b):
    try:
        result = a / b
        return result
    except ZeroDivisionError:
        print("Error: Division by zero!")
        return None
    except TypeError:
        print("Error: Invalid input types!")
        return None
    finally:
        print("Division operation attempted.")

# Using the function
result = safe_divide(10, 2)   # Returns 5.0
result = safe_divide(10, 0)   # Handles division by zero
    </div>
    
    <h3>File Input/Output</h3>
    
    <div class="code-block">
# Reading from files
with open('data.txt', 'r') as file:
    content = file.read()
    lines = content.split('\n')

# Writing to files
data = ["apple", "banana", "orange"]
with open('output.txt', 'w') as file:
    for item in data:
        file.write(f"{item}\n")

# Working with CSV files (preparation for pandas)
import csv

with open('data.csv', 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['Name', 'Age', 'City'])
    writer.writerow(['John', 25, 'New York'])
    writer.writerow(['Jane', 30, 'San Francisco'])
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Python Code Execution in Jupyter Notebook with Output]
    </div>
    
    <div class="page-number">19</div>
</div>

<!-- Page 20: Data Manipulation with Pandas -->
<div class="page">
    <h2>4.2 Data Manipulation with Pandas</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Pandas is the cornerstone library for data manipulation and analysis in Python. It provides powerful, flexible data structures and tools for working with structured data, making it indispensable for data science workflows.
    </p>
    
    <h3>Core Data Structures</h3>
    
    <h4>Series - One-dimensional labeled array</h4>
    <div class="code-block">
import pandas as pd
import numpy as np

# Creating Series
numbers = pd.Series([1, 2, 3, 4, 5])
labeled_series = pd.Series([10, 20, 30], index=['a', 'b', 'c'])
from_dict = pd.Series({'apple': 5, 'banana': 3, 'orange': 8})

# Series operations
print(numbers.mean())        # Calculate mean
print(numbers.std())         # Standard deviation
print(numbers.describe())    # Summary statistics
print(numbers[numbers > 3])  # Boolean indexing
    </div>
    
    <h4>DataFrame - Two-dimensional labeled data structure</h4>
    <div class="code-block">
# Creating DataFrames
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'Age': [25, 30, 35, 28],
    'City': ['New York', 'London', 'Tokyo', 'Paris'],
    'Salary': [50000, 60000, 75000, 55000]
}
df = pd.DataFrame(data)

# DataFrame from CSV
df_csv = pd.read_csv('employees.csv')

# DataFrame from dictionary of lists
df_dict = pd.DataFrame({
    'A': [1, 2, 3, 4],
    'B': [5, 6, 7, 8],
    'C': [9, 10, 11, 12]
})

# Basic information about DataFrame
print(df.shape)         # (rows, columns)
print(df.info())        # Data types and memory usage
print(df.describe())    # Summary statistics
print(df.head())        # First 5 rows
print(df.tail())        # Last 5 rows
    </div>
    
    <h3>Data Selection and Indexing</h3>
    
    <div class="code-block">
# Selecting columns
names = df['Name']              # Single column (Series)
subset = df[['Name', 'Age']]    # Multiple columns (DataFrame)

# Selecting rows
first_row = df.iloc[0]          # By position
row_by_label = df.loc[0]        # By label
multiple_rows = df.iloc[1:3]    # Slice by position

# Boolean indexing
high_earners = df[df['Salary'] > 55000]
young_high_earners = df[(df['Age'] < 30) & (df['Salary'] > 50000)]

# Setting new columns
df['Bonus'] = df['Salary'] * 0.1
df['Total_Compensation'] = df['Salary'] + df['Bonus']
df['City_Code'] = df['City'].map({
    'New York': 'NY', 'London': 'LDN', 'Tokyo': 'TKY', 'Paris': 'PAR'
})
    </div>
    
    <h3>Data Cleaning Operations</h3>
    
    <div class="code-block">
# Handling missing data
print(df.isnull().sum())        # Count missing values
df_cleaned = df.dropna()        # Remove rows with missing values
df_filled = df.fillna(0)        # Fill missing values with 0
df_forward_fill = df.fillna(method='ffill')  # Forward fill

# Removing duplicates
df_unique = df.drop_duplicates()
df_unique_names = df.drop_duplicates(subset=['Name'])

# Data type conversions
df['Age'] = df['Age'].astype('int32')
df['City'] = df['City'].astype('category')

# String operations
df['Name_Upper'] = df['Name'].str.upper()
df['Name_Length'] = df['Name'].str.len()
df['First_Name'] = df['Name'].str.split().str[0]
    </div>
    
    <h3>Data Aggregation and Grouping</h3>
    
    <div class="code-block">
# Basic aggregations
print(df['Salary'].mean())      # Average salary
print(df['Age'].median())       # Median age
print(df.describe())            # Summary statistics for all numeric columns

# GroupBy operations
city_stats = df.groupby('City').agg({
    'Salary': ['mean', 'max', 'min'],
    'Age': 'mean'
})

# Custom aggregation functions
def salary_range(x):
    return x.max() - x.min()

city_analysis = df.groupby('City').agg({
    'Salary': [salary_range, 'mean'],
    'Age': ['min', 'max']
})

# Value counts
city_counts = df['City'].value_counts()
    </div>
    
    <h3>Data Transformation</h3>
    
    <div class="code-block">
# Sorting data
df_sorted = df.sort_values('Salary', ascending=False)
df_multi_sort = df.sort_values(['City', 'Salary'], ascending=[True, False])

# Pivot tables
pivot_table = df.pivot_table(
    values='Salary',
    index='City',
    aggfunc=['mean', 'count']
)

# Melting data (wide to long format)
df_melted = pd.melt(df, 
                    id_vars=['Name', 'City'], 
                    value_vars=['Age', 'Salary'],
                    var_name='Metric', 
                    value_name='Value')

# Merging DataFrames
bonus_data = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Bonus_Rate': [0.15, 0.10, 0.12]
})

merged_df = pd.merge(df, bonus_data, on='Name', how='left')
    </div>
    
    <h3>Time Series Operations</h3>
    
    <div class="code-block">
# Working with dates
dates = pd.date_range('2025-01-01', periods=10, freq='D')
time_series = pd.Series(np.random.randn(10), index=dates)

# Date parsing
df['Date'] = pd.to_datetime(['2025-01-01', '2025-01-15', '2025-02-01', '2025-02-15'])
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day_of_Week'] = df['Date'].dt.day_name()

# Resampling time series data
monthly_avg = time_series.resample('M').mean()
    </div>
    
    <h3>Advanced Pandas Operations</h3>
    
    <table>
        <tr>
            <th>Operation</th>
            <th>Method</th>
            <th>Use Case</th>
        </tr>
        <tr>
            <td>Apply Functions</td>
            <td>.apply(), .applymap()</td>
            <td>Custom transformations across rows/columns</td>
        </tr>
        <tr>
            <td>Window Functions</td>
            <td>.rolling(), .expanding()</td>
            <td>Moving averages, cumulative calculations</td>
        </tr>
        <tr>
            <td>Categorical Data</td>
            <td>.astype('category')</td>
            <td>Memory optimization, ordered categories</td>
        </tr>
        <tr>
            <td>Multi-level Indexing</td>
            <td>.set_index(), .reset_index()</td>
            <td>Hierarchical data organization</td>
        </tr>
    </table>
    
    <div class="code-block">
# Apply functions example
df['Salary_Scaled'] = df.groupby('City')['Salary'].apply(
    lambda x: (x - x.mean()) / x.std()
)

# Rolling window calculations
df['Salary_Rolling_Mean'] = df['Salary'].rolling(window=2).mean()

# String operations on entire columns
df['Name_Initials'] = df['Name'].apply(
    lambda x: ''.join([word[0] for word in x.split()])
)
    </div>
    
    <h3>Performance Optimization Tips</h3>
    
    <div style="background: #f8f9fa; padding: 20px; margin: 20px 0; border-left: 4px solid #1f4788;">
        <ul>
            <li><strong>Use vectorized operations:</strong> Avoid loops, use pandas built-in methods</li>
            <li><strong>Choose appropriate data types:</strong> Use category for repeated strings</li>
            <li><strong>Read data efficiently:</strong> Use chunksize for large files</li>
            <li><strong>Index strategically:</strong> Set meaningful indices for faster lookups</li>
            <li><strong>Use query() method:</strong> More readable and sometimes faster filtering</li>
        </ul>
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Pandas DataFrame Operations in Jupyter Notebook]
    </div>
    
    <div class="page-number">20</div>
</div>

<!-- Page 21: Numerical Computing with NumPy -->
<div class="page">
    <h2>4.3 Numerical Computing with NumPy</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        NumPy (Numerical Python) is the foundation of the Python scientific computing stack. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays efficiently.
    </p>
    
    <h3>NumPy Arrays vs Python Lists</h3>
    
    <table>
        <tr>
            <th>Aspect</th>
            <th>NumPy Arrays</th>
            <th>Python Lists</th>
        </tr>
        <tr>
            <td>Performance</td>
            <td>Fast (implemented in C)</td>
            <td>Slower (interpreted Python)</td>
        </tr>
        <tr>
            <td>Memory Usage</td>
            <td>Efficient, contiguous memory</td>
            <td>Higher overhead</td>
        </tr>
        <tr>
            <td>Data Types</td>
            <td>Homogeneous (single type)</td>
            <td>Heterogeneous (mixed types)</td>
        </tr>
        <tr>
            <td>Operations</td>
            <td>Vectorized operations</td>
            <td>Element-by-element processing</td>
        </tr>
        <tr>
            <td>Functionality</td>
            <td>Rich mathematical functions</td>
            <td>Basic list operations</td>
        </tr>
    </table>
    
    <h3>Creating NumPy Arrays</h3>
    
    <div class="code-block">
import numpy as np

# From Python lists
arr1d = np.array([1, 2, 3, 4, 5])
arr2d = np.array([[1, 2, 3], [4, 5, 6]])

# Using NumPy functions
zeros = np.zeros((3, 4))           # Array of zeros
ones = np.ones((2, 3))             # Array of ones
full = np.full((2, 2), 7)          # Array filled with specific value
identity = np.eye(3)               # Identity matrix

# Sequences
range_arr = np.arange(0, 10, 2)    # [0, 2, 4, 6, 8]
linspace = np.linspace(0, 1, 5)    # 5 evenly spaced numbers from 0 to 1

# Random arrays
random_uniform = np.random.random((3, 3))        # Uniform [0,1)
random_normal = np.random.normal(0, 1, (2, 4))  # Normal distribution
random_int = np.random.randint(1, 10, (2, 3))   # Random integers

# Array properties
print(f"Shape: {arr2d.shape}")        # (2, 3)
print(f"Size: {arr2d.size}")          # 6
print(f"Dimensions: {arr2d.ndim}")    # 2
print(f"Data type: {arr2d.dtype}")    # int64
    </div>
    
    <h3>Array Indexing and Slicing</h3>
    
    <div class="code-block">
# 1D array indexing
arr = np.array([10, 20, 30, 40, 50])
print(arr[0])        # First element: 10
print(arr[-1])       # Last element: 50
print(arr[1:4])      # Slice: [20, 30, 40]

# 2D array indexing
arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print(arr2d[1, 2])   # Element at row 1, column 2: 6
print(arr2d[1])      # Entire row 1: [4, 5, 6]
print(arr2d[:, 1])   # Entire column 1: [2, 5, 8]
print(arr2d[1:, 1:]) # Subarray: [[5, 6], [8, 9]]

# Boolean indexing
mask = arr > 25
filtered = arr[mask]  # [30, 40, 50]

# Fancy indexing
indices = [0, 2, 4]
selected = arr[indices]  # [10, 30, 50]
    </div>
    
    <h3>Mathematical Operations</h3>
    
    <div class="code-block">
# Element-wise operations
a = np.array([1, 2, 3, 4])
b = np.array([5, 6, 7, 8])

# Basic arithmetic
addition = a + b      # [6, 8, 10, 12]
subtraction = a - b   # [-4, -4, -4, -4]
multiplication = a * b # [5, 12, 21, 32]
division = b / a      # [5.0, 3.0, 2.33, 2.0]
power = a ** 2        # [1, 4, 9, 16]

# Universal functions (ufuncs)
sqrt_vals = np.sqrt(a)           # Square root
exp_vals = np.exp(a)             # Exponential
log_vals = np.log(a)             # Natural logarithm
sin_vals = np.sin(a)             # Sine
abs_vals = np.abs(a)             # Absolute value

# Aggregate functions
print(f"Sum: {np.sum(a)}")           # 10
print(f"Mean: {np.mean(a)}")         # 2.5
print(f"Standard deviation: {np.std(a)}")  # 1.29
print(f"Minimum: {np.min(a)}")       # 1
print(f"Maximum: {np.max(a)}")       # 4
print(f"Median: {np.median(a)}")     # 2.5
    </div>
    
    <h3>Array Manipulation</h3>
    
    <div class="code-block">
# Reshaping arrays
arr = np.arange(12)
reshaped = arr.reshape(3, 4)    # 3x4 matrix
flattened = reshaped.flatten()   # Back to 1D

# Stacking arrays
a = np.array([[1, 2], [3, 4]])
b = np.array([[5, 6], [7, 8]])

vstack = np.vstack([a, b])      # Vertical stack
hstack = np.hstack([a, b])      # Horizontal stack
concatenate = np.concatenate([a, b], axis=0)  # Concatenate along axis

# Splitting arrays
arr = np.arange(16).reshape(4, 4)
split_arrays = np.split(arr, 2, axis=0)  # Split into 2 parts along rows

# Transposing
transposed = arr.T
# or
transposed = np.transpose(arr)
    </div>
    
    <h3>Linear Algebra Operations</h3>
    
    <div class="code-block">
# Matrix operations
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Matrix multiplication
matrix_mult = np.dot(A, B)
# or using @ operator
matrix_mult = A @ B

# Linear algebra functions
determinant = np.linalg.det(A)
inverse = np.linalg.inv(A)
eigenvalues, eigenvectors = np.linalg.eig(A)

# Solving linear equations: Ax = b
b = np.array([1, 2])
x = np.linalg.solve(A, b)
    </div>
    
    <h3>Broadcasting</h3>
    
    <p style="text-align: justify;">
        Broadcasting allows NumPy to perform operations on arrays with different shapes without explicitly reshaping them:
    </p>
    
    <div class="code-block">
# Broadcasting examples
arr = np.array([[1, 2, 3],
                [4, 5, 6],
                [7, 8, 9]])

# Add scalar to all elements
result1 = arr + 10

# Add 1D array to 2D array
row_add = np.array([10, 20, 30])
result2 = arr + row_add  # Adds to each row

# Add column vector
col_add = np.array([[100], [200], [300]])
result3 = arr + col_add  # Adds to each column
    </div>
    
    <h3>Performance Comparison Example</h3>
    
    <div class="code-block">
import time

# Performance comparison: NumPy vs Pure Python
size = 1000000

# Pure Python
start = time.time()
python_list = list(range(size))
python_result = [x**2 for x in python_list]
python_time = time.time() - start

# NumPy
start = time.time()
numpy_array = np.arange(size)
numpy_result = numpy_array**2
numpy_time = time.time() - start

print(f"Python time: {python_time:.4f} seconds")
print(f"NumPy time: {numpy_time:.4f} seconds")
print(f"NumPy is {python_time/numpy_time:.1f}x faster")
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: NumPy Array Operations Performance Benchmark Results]
    </div>
    
    <div class="page-number">21</div>
</div>

<!-- Page 22: Data Visualization -->
<div class="page">
    <h1>5. DATA VISUALIZATION</h1>
    
    <p style="text-align: justify; margin-top: 30px;">
        Data visualization is the graphical representation of information and data. It uses visual elements like charts, graphs, and maps to provide an accessible way to see and understand trends, outliers, and patterns in data. Effective visualization is crucial for exploratory data analysis, hypothesis formation, and communicating insights to stakeholders.
    </p>
    
    <h2>The Importance of Data Visualization</h2>
    
    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin: 20px 0;">
        <div style="background: #e3f2fd; padding: 20px; border-radius: 8px;">
            <h3>For Data Scientists</h3>
            <ul>
                <li><strong>Pattern Recognition:</strong> Quickly identify trends and anomalies</li>
                <li><strong>Data Quality:</strong> Spot missing values and outliers</li>
                <li><strong>Hypothesis Generation:</strong> Visual insights suggest new questions</li>
                <li><strong>Model Validation:</strong> Assess model performance visually</li>
            </ul>
        </div>
        <div style="background: #f3e5f5; padding: 20px; border-radius: 8px;">
            <h3>For Stakeholders</h3>
            <ul>
                <li><strong>Clear Communication:</strong> Complex data made understandable</li>
                <li><strong>Decision Support:</strong> Visual evidence for business decisions</li>
                <li><strong>Engagement:</strong> Interactive dashboards maintain interest</li>
                <li><strong>Accessibility:</strong> Non-technical audiences can understand</li>
            </ul>
        </div>
    </div>
    
    <h2>Visualization Types and Use Cases</h2>
    
    <table>
        <tr>
            <th>Chart Type</th>
            <th>Primary Purpose</th>
            <th>Data Requirements</th>
            <th>Best For</th>
        </tr>
        <tr>
            <td>Bar Chart</td>
            <td>Compare categories</td>
            <td>Categorical + Numerical</td>
            <td>Discrete comparisons</td>
        </tr>
        <tr>
            <td>Line Chart</td>
            <td>Show trends over time</td>
            <td>Time series data</td>
            <td>Continuous changes</td>
        </tr>
        <tr>
            <td>Scatter Plot</td>
            <td>Show relationships</td>
            <td>Two numerical variables</td>
            <td>Correlation analysis</td>
        </tr>
        <tr>
            <td>Histogram</td>
            <td>Show distribution</td>
            <td>Single numerical variable</td>
            <td>Understanding data spread</td>
        </tr>
        <tr>
            <td>Box Plot</td>
            <td>Compare distributions</td>
            <td>Numerical data by groups</td>
            <td>Identifying outliers</td>
        </tr>
        <tr>
            <td>Heat Map</td>
            <td>Show correlation/intensity</td>
            <td>Matrix data</td>
            <td>Pattern recognition</td>
        </tr>
    </table>
    
    <h2>The Visualization Process</h2>
    
    <div class="process-flow">
        <div class="process-step">Understand Data</div>
        <div class="arrow">→</div>
        <div class="process-step">Choose Chart Type</div>
        <div class="arrow">→</div>
        <div class="process-step">Design Visual</div>
        <div class="arrow">→</div>
        <div class="process-step">Add Context</div>
        <div class="arrow">→</div>
        <div class="process-step">Refine & Polish</div>
    </div>
    
    <h2>Python Visualization Ecosystem</h2>
    
    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin: 20px 0;">
        <div style="background: #e8f5e8; padding: 15px; border-radius: 8px; text-align: center;">
            <h4>Matplotlib</h4>
            <p>Foundation library</p>
            <p>Maximum customization</p>
            <p>Static plots</p>
        </div>
        <div style="background: #ffe8e8; padding: 15px; border-radius: 8px; text-align: center;">
            <h4>Seaborn</h4>
            <p>Statistical visualization</p>
            <p>Beautiful defaults</p>
            <p>High-level interface</p>
        </div>
        <div style="background: #e8e8ff; padding: 15px; border-radius: 8px; text-align: center;">
            <h4>Plotly</h4>
            <p>Interactive plots</p>
            <p>Web-ready</p>
            <p>3D visualization</p>
        </div>
    </div>
    
    <h2>Design Principles for Effective Visualization</h2>
    
    <h3>1. Clarity and Simplicity</h3>
    <ul>
        <li>Remove unnecessary elements (chartjunk)</li>
        <li>Use clear, descriptive titles and labels</li>
        <li>Maintain appropriate aspect ratios</li>
        <li>Choose readable fonts and sizes</li>
    </ul>
    
    <h3>2. Accuracy and Integrity</h3>
    <ul>
        <li>Start y-axis at zero for bar charts (when appropriate)</li>
        <li>Use consistent scales across related charts</li>
        <li>Avoid misleading visual encodings</li>
        <li>Provide context for data interpretation</li>
    </ul>
    
    <h3>3. Color Usage</h3>
    <ul>
        <li>Use color purposefully, not decoratively</li>
        <li>Consider color-blind accessibility</li>
        <li>Maintain sufficient contrast</li>
        <li>Use consistent color schemes</li>
    </ul>
    
    <h3>4. Audience Consideration</h3>
    <ul>
        <li>Match complexity to audience expertise</li>
        <li>Include necessary context and explanations</li>
        <li>Consider cultural and contextual factors</li>
        <li>Enable interaction when beneficial</li>
    </ul>
    
    <div class="data-viz">
        Good Visualization = Clear Message + Appropriate Chart + Thoughtful Design
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Gallery of Different Chart Types with Sample Data]
    </div>
    
    <div class="page-number">22</div>
</div>

<!-- Page 23: Visualization Principles -->
<div class="page">
    <h2>5.1 Visualization Principles</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Effective data visualization requires more than just technical skills—it demands understanding of visual perception, cognitive psychology, and design principles. This section explores the foundational principles that guide the creation of clear, accurate, and compelling data visualizations.
    </p>
    
    <h3>The Grammar of Graphics</h3>
    
    <p style="text-align: justify;">
        The Grammar of Graphics, developed by Leland Wilkinson, provides a systematic approach to creating visualizations by breaking them down into fundamental components:
    </p>
    
    <table>
        <tr>
            <th>Component</th>
            <th>Description</th>
            <th>Examples</th>
        </tr>
        <tr>
            <td>Data</td>
            <td>The dataset being visualized</td>
            <td>Sales figures, survey responses, sensor readings</td>
        </tr>
        <tr>
            <td>Aesthetics</td>
            <td>Visual properties mapped to data</td>
            <td>Position (x,y), color, size, shape</td>
        </tr>
        <tr>
            <td>Geometries</td>
            <td>Geometric objects representing data</td>
            <td>Points, lines, bars, areas</td>
        </tr>
        <tr>
            <td>Scales</td>
            <td>Mapping between data and aesthetics</td>
            <td>Linear, logarithmic, categorical scales</td>
        </tr>
        <tr>
            <td>Facets</td>
            <td>Subplots based on data subsets</td>
            <td>Small multiples, panel plots</td>
        </tr>
        <tr>
            <td>Statistics</td>
            <td>Data transformations</td>
            <td>Smoothing, binning, aggregation</td>
        </tr>
    </table>
    
    <h3>Visual Encoding Principles</h3>
    
    <h4>Hierarchy of Visual Encodings</h4>
    <p style="text-align: justify;">
        Different visual properties have varying effectiveness for encoding quantitative data:
    </p>
    
    <div style="background: #f8f9fa; padding: 20px; margin: 20px 0; border-left: 4px solid #1f4788;">
        <h5>Most Effective (Quantitative Data):</h5>
        <ol>
            <li><strong>Position:</strong> Most accurate for comparing values</li>
            <li><strong>Length:</strong> Effective for showing magnitude</li>
            <li><strong>Angle/Slope:</strong> Good for showing direction of change</li>
        </ol>
        
        <h5>Moderately Effective:</h5>
        <ol start="4">
            <li><strong>Area:</strong> Useful but can be misjudged</li>
            <li><strong>Volume:</strong> Difficult to assess accurately</li>
            <li><strong>Color Intensity:</strong> Good for continuous data</li>
        </ol>
        
        <h5>Least Effective (Quantitative Data):</h5>
        <ol start="7">
            <li><strong>Color Hue:</strong> Better for categorical data</li>
            <li><strong>Texture:</strong> Difficult to order quantitatively</li>
        </ol>
    </div>
    
    <h3>Preattentive Processing</h3>
    
    <p style="text-align: justify;">
        Preattentive attributes are visual properties that can be perceived rapidly (under 250ms) without conscious attention. Leveraging these attributes makes visualizations more intuitive:
    </p>
    
    <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin: 20px 0;">
        <div style="background: #e3f2fd; padding: 15px; border-radius: 8px;">
            <h4>Color Attributes</h4>
            <ul>
                <li>Hue (red vs. blue)</li>
                <li>Intensity (light vs. dark)</li>
                <li>Saturation (vivid vs. muted)</li>
            </ul>
        </div>
        <div style="background: #f3e5f5; padding: 15px; border-radius: 8px;">
            <h4>Form Attributes</h4>
            <ul>
                <li>Size (small vs. large)</li>
                <li>Shape (circle vs. square)</li>
                <li>Orientation (vertical vs. horizontal)</li>
            </ul>
        </div>
        <div style="background: #e8f5e8; padding: 15px; border-radius: 8px;">
            <h4>Motion Attributes</h4>
            <ul>
                <li>Direction of movement</li>
                <li>Speed of movement</li>
                <li>Flickering</li>
            </ul>
        </div>
        <div style="background: #fff3e0; padding: 15px; border-radius: 8px;">
            <h4>Spatial Attributes</h4>
            <ul>
                <li>Position</li>
                <li>Grouping (proximity)</li>
                <li>Added marks (enclosure)</li>
            </ul>
        </div>
    </div>
    
    <h3>Common Visualization Mistakes</h3>
    
    <h4>1. Misleading Scales</h4>
    <div class="code-block">
# Example: Truncated y-axis can exaggerate differences
import matplotlib.pyplot as plt
import numpy as np

# Data
categories = ['A', 'B', 'C', 'D']
values = [100, 102, 101, 103]

# Misleading: Truncated y-axis
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.bar(categories, values)
plt.ylim(99, 104)  # Truncated y-axis
plt.title('Misleading: Truncated Y-axis')

# Better: Full scale or clear indication of break
plt.subplot(1, 2, 2)
plt.bar(categories, values)
plt.ylim(0, 110)  # Full scale
plt.title('Better: Full Scale Context')

plt.tight_layout()
plt.show()
    </div>
    
    <h4>2. Inappropriate Chart Types</h4>
    <ul>
        <li><strong>Pie charts with too many slices:</strong> Use bar charts instead</li>
        <li><strong>3D effects without purpose:</strong> Often distort perception</li>
        <li><strong>Dual y-axes:</strong> Can be misleading if scales aren't carefully chosen</li>
        <li><strong>Area charts for non-cumulative data:</strong> Use line charts instead</li>
    </ul>
    
    <h3>Accessibility in Data Visualization</h3>
    
    <h4>Color Accessibility</h4>
    <div class="code-block">
# Using colorbrewer palettes for accessibility
import seaborn as sns

# Color-blind friendly palettes
colorblind_palette = sns.color_palette("colorblind")
sns.set_palette("colorblind")

# Alternative: use patterns or shapes in addition to color
plt.figure(figsize=(10, 6))
x = np.arange(4)
plt.bar(x, [1, 2, 3, 4], color='blue', alpha=0.7, label='Series 1')
plt.bar(x + 0.4, [1.2, 1.8, 2.9, 3.8], color='red', alpha=0.7, 
        hatch='///', label='Series 2')
plt.legend()
plt.title('Accessible Bar Chart with Patterns')
    </div>
    
    <h4>Text and Typography</h4>
    <ul>
        <li><strong>Font size:</strong> Minimum 12pt for body text, larger for titles</li>
        <li><strong>Contrast:</strong> High contrast between text and background</li>
        <li><strong>Readability:</strong> Sans-serif fonts often better for screen display</li>
        <li><strong>Alt text:</strong> Provide descriptive text for screen readers</li>
    </ul>
    
    <h3>Cultural and Contextual Considerations</h3>
    
    <div style="background: #fff3cd; padding: 20px; margin: 20px 0; border: 1px solid #ffeaa7; border-radius: 8px;">
        <h4>Global Visualization Considerations:</h4>
        <ul>
            <li><strong>Color meanings:</strong> Red might mean danger in some cultures, luck in others</li>
            <li><strong>Reading direction:</strong> Left-to-right vs. right-to-left affects layout</li>
            <li><strong>Number formats:</strong> Decimal separators and digit grouping vary</li>
            <li><strong>Date formats:</strong> MM/DD/YYYY vs. DD/MM/YYYY vs. YYYY-MM-DD</li>
            <li><strong>Cultural symbols:</strong> Icons and metaphors may not translate</li>
        </ul>
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Before and After Examples of Visualization Improvements]
    </div>
    
    <div class="page-number">23</div>
</div>

<!-- Page 24: Matplotlib and Seaborn -->
<div class="page">
    <h2>5.2 Matplotlib and Seaborn</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Matplotlib is the foundational plotting library in Python, providing low-level control over visualizations. Seaborn builds on Matplotlib to offer high-level statistical plotting functions with attractive default styles.
    </p>
    
    <h3>Matplotlib Basics</h3>
    
    <div class="code-block">
import matplotlib.pyplot as plt
import numpy as np

# Simple line plot
x = np.linspace(0, 10, 100)
y = np.sin(x)
plt.plot(x, y)
plt.title('Sine Wave')
plt.xlabel('X')
plt.ylabel('sin(X)')
plt.grid(True)
plt.show()
    </div>
    
    <h3>Seaborn Statistical Plots</h3>
    
    <div class="code-block">
import seaborn as sns
import pandas as pd

# Load sample dataset
tips = sns.load_dataset("tips")

# Scatter plot with regression
sns.regplot(x="total_bill", y="tip", data=tips)
plt.show()

# Distribution plot
sns.histplot(tips["total_bill"], kde=True)
plt.show()
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Matplotlib and Seaborn Plot Examples]
    </div>
    
    <div class="page-number">24</div>
</div>

<!-- Page 25: Interactive Visualizations -->
<div class="page">
    <h2>5.3 Interactive Visualizations</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Interactive visualizations allow users to explore data dynamically, providing deeper insights through user interaction.
    </p>
    
    <h3>Plotly Basics</h3>
    
    <div class="code-block">
import plotly.express as px

df = px.data.iris()
fig = px.scatter(df, x="sepal_width", y="sepal_length", color="species")
fig.show()
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Interactive Plotly Visualization]
    </div>
    
    <div class="page-number">25</div>
</div>

<!-- Page 26: Machine Learning Basics -->
<div class="page">
    <h1>6. MACHINE LEARNING BASICS</h1>
    
    <p style="text-align: justify; margin-top: 30px;">
        Machine learning is a subset of artificial intelligence that enables systems to learn from data and improve performance without explicit programming.
    </p>
    
    <div class="screenshot-placeholder">
        [Screenshot: Machine Learning Overview Diagram]
    </div>
    
    <div class="page-number">26</div>
</div>

<!-- Page 27: Introduction to ML -->
<div class="page">
    <h2>6.1 Introduction to ML</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Machine learning algorithms build mathematical models based on sample data to make predictions or decisions.
    </p>
    
    <div class="code-block">
from sklearn.datasets import load_iris
iris = load_iris()
print(iris.data.shape)
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: ML Algorithm Flowchart]
    </div>
    
    <div class="page-number">27</div>
</div>

<!-- Page 28: Supervised Learning -->
<div class="page">
    <h2>6.2 Supervised Learning</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Supervised learning uses labeled data to train models for classification or regression tasks.
    </p>
    
    <div class="code-block">
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)
model = LogisticRegression()
model.fit(X_train, y_train)
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Supervised Learning Example]
    </div>
    
    <div class="page-number">28</div>
</div>

<!-- Page 29: Unsupervised Learning -->
<div class="page">
    <h2>6.3 Unsupervised Learning</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Unsupervised learning finds patterns in unlabeled data, such as clustering or dimensionality reduction.
    </p>
    
    <div class="code-block">
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3)
kmeans.fit(iris.data)
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Clustering Visualization]
    </div>
    
    <div class="page-number">29</div>
</div>

<!-- Page 30: Capstone Project Overview -->
<div class="page">
    <h1>7. CAPSTONE PROJECT</h1>
    
    <h2>7.1 Project Overview</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        The capstone project demonstrates the practical application of data science concepts learned throughout the IBM Data Science Foundations Level 1 certification. The project involves analyzing a dataset to derive business insights using various techniques covered in the course.
    </p>
    
    <h3>Project Description</h3>
    
    <p style="text-align: justify;">
        For this capstone, we analyze the Iris dataset to classify flower species using machine learning algorithms. This project encompasses data loading, exploration, visualization, model training, and evaluation.
    </p>
    
    <h3>Objectives</h3>
    
    <ul>
        <li>Load and explore the dataset</li>
        <li>Perform exploratory data analysis</li>
        <li>Visualize data distributions and relationships</li>
        <li>Train and evaluate classification models</li>
        <li>Interpret results and draw conclusions</li>
    </ul>
    
    <div class="screenshot-placeholder">
        [Screenshot: Project Dataset Overview]
    </div>
    
    <div class="page-number">30</div>
</div>

<!-- Page 31: Data Analysis Process -->
<div class="page">
    <h2>7.2 Data Analysis Process</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        The analysis process follows the CRISP-DM methodology, starting from data understanding to modeling and evaluation.
    </p>
    
    <div class="code-block">
from sklearn.datasets import load_iris
import pandas as pd

iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['species'] = iris.target
print(df.describe())
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Data Exploration Output]
    </div>
    
    <div class="page-number">31</div>
</div>

<!-- Page 32: Results and Insights -->
<div class="page">
    <h2>7.3 Results and Insights</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        The model achieved high accuracy in classifying iris species, with petal features being the most discriminative.
    </p>
    
    <div class="code-block">
from sklearn.metrics import accuracy_score

predictions = model.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f"Accuracy: {accuracy * 100:.2f}%")
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Model Performance Metrics]
    </div>
    
    <div class="page-number">32</div>
</div>

<!-- Page 33: Project Screenshots and Code -->
<div class="page">
    <h2>7.4 Project Screenshots and Code</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        This page includes additional screenshots and code snippets from the capstone project.
    </p>
    
    <div class="screenshot-placeholder">
        [Screenshot: Confusion Matrix]
    </div>
    
    <div class="screenshot-placeholder">
        [Screenshot: Feature Importance Plot]
    </div>
    
    <div class="page-number">33</div>
</div>

<!-- Page 34: Additional Project Details -->
<div class="page">
    <h2>7.5 Additional Project Details</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Detailed discussion of model selection and hyperparameter tuning.
    </p>
    
    <div class="code-block">
from sklearn.model_selection import GridSearchCV

param_grid = {'C': [0.1, 1, 10]}
grid = GridSearchCV(LogisticRegression(), param_grid)
grid.fit(X_train, y_train)
    </div>
    
    <div class="page-number">34</div>
</div>

<!-- Page 35: Conclusion & Future Learning -->
<div class="page">
    <h1>8. CONCLUSION & FUTURE LEARNING</h1>
    
    <p style="text-align: justify; margin-top: 30px;">
        This certification has provided a solid foundation in data science principles and practices.
    </p>
    
    <div class="page-number">35</div>
</div>

<!-- Page 36: Key Takeaways -->
<div class="page">
    <h2>8.1 Key Takeaways</h2>
    
    <p style="text-align: justify; margin-top: 30px;">
        Summary of main lessons learned and skills acquired.
    </p>
    
    <ul>
        <li>Statistical analysis skills</li>
        <li>Programming proficiency</li>
        <li>Machine learning basics</li>
    </ul>
    
    <div class="page-number">36</div>
</div>

<!-- Page 37: References -->
<div class="page">
    <h1>9. REFERENCES</h1>
    
    <p style="text-align: justify; margin-top: 30px;">
        List of references used in this report.
    </p>
    
    <ul>
        <li>IBM Data Science Course Materials</li>
        <li>Python Documentation</li>
        <li>Scikit-learn Tutorials</li>
        <li>Pandas User Guide</li>
        <li>Matplotlib Documentation</li>
    </ul>
    
    <div class="page-number">37</div>
</div>

<!-- Page 38: Appendices -->
<div class="page">
    <h1>10. APPENDICES</h1>
    
    <p style="text-align: justify; margin-top: 30px;">
        Additional supporting materials.
    </p>
    
    <div class="page-number">38</div>
</div>

<!-- Page 39: Appendix A: Additional Code Samples -->
<div class="page">
    <h2>Appendix A: Additional Code Samples</h2>
    
    <div class="code-block">
# Additional code sample
print("Hello, Data Science!")
    </div>
    
    <div class="page-number">39</div>
</div>

<!-- Page 40: Appendix B: Glossary -->
<div class="page">
    <h2>Appendix B: Glossary</h2>
    
    <ul>
        <li><strong>Data Science:</strong> Multidisciplinary field for extracting insights from data</li>
        <li><strong>Machine Learning:</strong> Algorithms that learn from data</li>
        <li><strong>Statistics:</strong> Mathematical analysis of data</li>
        <li><strong>Visualization:</strong> Graphical representation of data</li>
    </ul>
    
    <div class="page-number">40</div>
</div>

</body>
</html>
